Hello, everybody!
My name is Benoit
Dherin, a machine learning engineer at Google's Advanced Solutions Lab.
If you want to know more about what the Advanced Solutions Lab is, please
follow the link below in the description box.
There is lots of excitement currently around generative AI and new advancements,
including new Vertex AI features such as GenAI
Studio, Model Garden, GenAI API.
Our objective in this short session is to give you a solid footing
on some of the underlying concepts that make all the Gen AI magic possible.
Today, I’ll go over the code that’s complementary
to the “Encoder-Decoder Architecture Overview” course in the same series.
We will see together how to build a poetry generator
from scratch using the encoder-decoder architecture.
Using the angular decoder architecture
you’d find to set up instructions in our GitHub repository.
Okay, let's now have a look at the code.
To access our lab, go in the asl-ml-immersion folder.
Then the notebooks folder.
Then the text_models folder.
And in the solutions from there you'll find the text generation notebook.
That's the lab that we'll cover today.
In this lab we will implement a character based text generator
based on the encoder decoder architecture.
Character based means that the tokens consumed
and generated by the network are characters and not words.
We will use plays as a data set.
They have a special structure
which are that of people talking with each other.
And here you see an example of a piece of text
that has been generated by the trained neural network.
When the sentences are not necessarily making sense
nor are grammatically correct.
This is remarkable in many ways.
First of all, remember, it's character based.
So it means that it learns to predict only the most probable characters.
Despite that, it was able to learn pretty well
The notion of words separated by blank spaces.
And also the basic structure of a play with the characters talking to each other.
So going
of what is a very small network, as you will see, it's based on the rnn
0 and architecture and only trained for 30 epochs in the vertex
air workbench, which is a pretty fast training time.
So let's look at the code now.
So the first thing is to import the libraries that we need.
In particular, we could or encoder decoder architecture
using TensorFlow Keras to impart that.
Then we download our data set
using tf.keras.utils.get_file.
So now the dataset is on disk and we just need to load it into a variable called text.
So the text variable now contains the whole string representing
all the all the plays in that Shakespeare dataset.
Can I have a quick look at what it is?
And you see if we printed the first 250 characters.
You have the first citizens speaking to
everybody and everybody else is speaking
to the first citizen.
The cell computes the number of unique characters that we have in that
in the text dataset, and we see that we have
65 unique characters, right?
These characters would be the tokens that the neural network will consume
during training and will generating during this service.
So the first step here
now is to vectorize the text.
What do we mean by that?
It means that first of all,
we will need to extract
from the actual string sequence of characters, which we can do
with TensorFlow by using tf.strings.unicode_split.
So now, for example, texts
here are transformed into a list
of sequences of characters.
A neural network cannot consume immediately.
The characters.
We need to transform that into numbers.
So we need to simply map each of the characters to a given
id. For that we have the
tf.keras.layers.StringLookup
to which you just need to pass to
the list of your vocabulary.
The 65 unique character that we have in our corpus
and that we produce a layer that when passed the characters
will produce corresponding ids.
So within that, that layer you have a mapping that has been generated
between the characters and the
id. To get the inverse mapping,
you use the same layer of string lookup
with the exact same vocabulary that you retrieve
from the first of the year by using get vocabulary.
But you set that parameter to be true, invert, equal equal true,
and that will compute the invert mapping, which is the mapping from
id to chars
Right.
And indeed, if you pass to this mapping
sequence of ID’s, the ID’s,
it gives you back
the corresponding characters.
Using the mapping that start in the memory of this layer.
So that's that.
Okay.
Now let's that's been the dataset that we will train our neural network with.
For that we are using the
tf.data.Dataset API,
which has this nice method
from tons of slices
which will convert.
That answer of instance represents or whole corpus of text of plays
as id it will store that into it to have data data sets.
So at this point, the elements of these datasets
are just the individual characters.
So that's not great for us.
But we want to feed our neural network with our sequences
of the same length but not just one character.
We need to predict the next character. So
but luckily the
dataset API has this nice function batch that will do exactly that for us.
So if we pass, if we invoke the batch
method on our ID dataset,
to which we pass a given sequence length,
which we said to be 100 here, now the elements,
the data points that are stored in our dataset
are no longer characters, but the sequences of
100 characters.
So here you see an example.
If we take just one element, they are no longer characters,
but sequences of hundreds of their character IDs
0 you want not characters, but character IDs.
Okay, it's
not completely we are not completely done here.
We still need to create
the input sequences that we were going to pass to the decoder
and also the sequences that we want to predict.
Right?
And what are the sequences that are just the sequences of the next character
in the input sequence?
So for instance, here, if we have the sequence TensorFlow
and the sequence TensorFlow at the beginning,
then the input sequence we can do from
it is tens-or-flow, We know the W
and the target sequence that we want to predict
is the same sequence, but just shifted by one
on the right, so ensor-low and
you see that E is the next character for Ring T
and is the next there for E, etc.
So basically this little function does exactly that.
It takes an original sequence, creates
an input sequence from that by just truncating that sequence
0 removing the last character and that just the target sequence is created
by started at starting add the first character.
So how we do that, we just map
the split input target function to our sequence dataset.
Okay.
And it's already does it.
Now let's see how to build the model.
First off, we set a number of variables
the vocabulary size, the size of the vectors.
We want to represent the characters will I think That would be 256
and a number of neurons or recurrent layer we'd have.
For the model itself.
It's a relatively simple model.
We create it by using the Keras subclass API.
We create just a new class called MyModel
and we subclass here
from tf.keras.Model.
When you do that you only have to
override two functions, the constructor and the call function.
So let's see what each of these function does.
The first function takes essentially the hyper parameters of your model,
the vocabulary size, the embedding dimension, the number of neuron
that number of neurons for your recurrent layer,
and it just constricts the layers you will need and store them
as variables of the class.
Okay.
Now really
how these layers are connected,
all that is specified in the call function,
the architecture of your network, if you will.
If you want.
Let's see where to the body does. Here.
Take the input which are sequences of ids representing the characters.
We have a first layer that we'll create for each of the
inits a vector representing that.
So that's the training layer.
So as the training progresses, this vector is representing the characters.
We'll start to be more and more meaningful.
At least that's the idea.
Then these static representations of the characters
is passed to the recurrent layer that we'll somehow
modify these for representation
according to the context of what I've seen with what has been seen
previously and generate a state of
what is seen previously, that would be a reuse in the next step.
Finally, we pass the output of the
recurrent layer to a dense layer that will output
as many numbers that we as we have in our vocabulary,
which means one
score for each
of the possible 65 characters and the score
represent the probability of the character
being the next one.
So that's all that the model does.
Then we instantiated.
Once we have done that, we can look at the structure of the model
using model summary, and you see here you have the I'm building the year,
the recurrently year and the dust layer that we just encoded
implemented in our, in our model
does that.
So let's train the model.
Before we train the model, we need a loss and that's the loss function
that we compared the output of the model with the truth, right?
Since that's essentially a classification problem with many classes
and the classes being
each of the possible characters to be the next,
the loss would be the SparseCategoricalCrossentropy loss.
And also because the neural network
output, the logits are not directly the probability we configure this loss
to be computed not from the probability scores,
but from the logits scores.
Okay,
once we have the loss, we can combine our model,
which means that basically we tied to it a loss and also an optimizer.
And that will update the weights during training
to decrease the loss as much as possible.
Basically, it
then here we have a little bit of a callback that we will use
and that will save the weights during training,
which is a useful
item.
And we are all set up now to start the training.
So we do a model.fit on the data set.
We choose a number of epochs we want to be trained on.
An epoch is a full pass on the data set.
So here we we have a look at ten, ten times
the corpus of plays we have in our text vector
and we give the callback to make sure
that the weights are saved during the the training,
that's it.
So that's relatively simple. We train them my data.
We have a train model now what do we do with it?
And that's a bit of a complication in the
encoder.
Decoder architecture is that you cannot through the immediately use your model,
you need to write a sort of a decoding function that's here
that will decode the generated text
a step at a time using the trained model.
Okay.
So here in this case, we chose to
implement this decoding function
as a Keras model.
So we subclass from the tf.keras.Model.
The main method
in that model is to generate one step.
It's a quick look to what it does,
so it takes the inputs so the input can be
to prompt the initial prompt initial the sequence of character you want to
the encoder-decoder model to
complete, to predict, to generate new new characters.
So you bypass the input it
transform that text into a sequence of character,
and then the sequence of characters into a sequence of ids.
Using the ids_from_chars.
Here we have a setup previously, and then we call our model
or encoder-decoder model that has been previously trained.
And what does it do?
It takes this input of ids and
output the predicted logits.
So this calls for the most probable token the most probable character in this case,
0 along with the state that summarizes what has been seen previously.
From the predicted logits, we can compute, we can select
the most likely tokens or characters.
But before doing that there is a little bit of a trick,
which is that we divide the logits by a temperature, by a number.
So basically if the temperature is one, nothing happens.
But if the temperature is very high,
what it will do, it will makes the scores
associated to each of the token to be predictive.
Next will be relatively similar, close to zero.
This means that actually
this token would be more and more likely to be chosen, right?
So there would be more variety, more a more stuff can be predicted
if the temperature is higher.
So it's a bit more creative If you have a two high temperature,
of course, the neural network would just predict the gibberish.
Okay.
And if you have a true temperature, the highest probability
score will be just multiply by a very large number
because it's divided by a small number, it's a number between zero and one,
which means that the highest score will be
become much, much
bigger than the other scores, giving a much higher chance
to be selected, which gives you more of the deterministic behavior.
Okay, that's the temperature.
That's an important parameter, as in this type of architecture.
Okay.
And that's what it does.
Okay.
So now we have the predicted logits we use tf.random.categorical
to just sample from these probability scores
the most likely idea is to be next.
We transform that back to a character and that's what we return.
Okay, So that's essentially what the decoding function does
and most decoding function at the very same structure.
There is also this temperature trick that you can see as a
as a parameter in the case of large
language models.
Okay, so let's use our decoding function.
So typically you use that in the loop.
So here we are going to predict 1000 characters by repeatedly
making a call to the decoding function generated one step,
to which you feed
what has been predicted
before, along with the state summarizing what happened before,
and it predict the next character along with a new state.
And we start the process.
We do sort of a prompt here. That's
Romeo.
What are you going to say? And then the
there are let's let's see what the neuron that generates, right?
Says no good corona at least take your feetle
and if I seem to my love you...
so you see it's not it doesn't make a lot of sense here.
0 Remember I've trained it only a few minutes
on the work bench,
AI work bench in Vertex AI Workbench, which are great by the way, but here
that's a small instance which just one GPU So it was a very small training.
The model is written
in a few lines, but yet you still see that it can really pick up
a lot of things in the structure of the of the input data.
It detects patterns that you have characters.
So Romeo, that was our input,
but then
Leontes was generated by the network and then what
Leontes says. So
okay,
that's it.
If you like this presentation, you'll find more on our ASL
GitHub repository with 90 plus machine learning and notebooks.
Don't forget it.
If you find it useful, please star our repo.
Thanks for your time.