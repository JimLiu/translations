1
00:00:00,330 --> 00:00:00,996
Welcome back.

2
00:00:00,996 --> 00:00:04,907
There's a lot of exciting material to go over this week, and

3
00:00:04,907 --> 00:00:09,661
one of the first topics that Mike will share with you in a little bit is a deep

4
00:00:09,661 --> 00:00:13,252
dive into how transformer networks actually work.

5
00:00:13,252 --> 00:00:16,532
Yeah, so look, it's a complicated topic, right?

6
00:00:16,532 --> 00:00:20,030
In 2017, the paper came out, Tension is all You Need, and

7
00:00:20,030 --> 00:00:24,254
it laid out all of these fairly complex data processes which are going to happen

8
00:00:24,254 --> 00:00:26,594
inside the transformer architecture.

9
00:00:26,594 --> 00:00:31,298
So we take a little bit of a high level view, but we do go down into some depths.

10
00:00:31,298 --> 00:00:33,682
We talk about things like self-attention and

11
00:00:33,682 --> 00:00:36,354
the multi-headed self-attention mechanism.

12
00:00:36,354 --> 00:00:40,316
So we can see why it is that these models actually work,

13
00:00:40,316 --> 00:00:45,126
how it is that they actually gain an understanding of language.

14
00:00:45,126 --> 00:00:48,870
And it's amazing how long the transformer architecture has been around

15
00:00:48,870 --> 00:00:51,494
and it's still state of the art for many models.

16
00:00:51,494 --> 00:00:56,289
I remember after I saw the transformer paper when it first came out, I thought,

17
00:00:56,289 --> 00:00:58,058
yep, I get this equation.

18
00:00:58,058 --> 00:01:00,538
I acknowledge this is a math equation.

19
00:01:00,538 --> 00:01:02,500
But what's it actually doing?

20
00:01:02,500 --> 00:01:04,154
And it's always seemed a little bit magical.

21
00:01:04,154 --> 00:01:08,056
It took me a long time playing with it to finally go, okay, this is why it works.

22
00:01:08,056 --> 00:01:12,106
And so I think in this first week, you learn the intuitions behind

23
00:01:12,106 --> 00:01:17,102
some of these terms you may have heard before, like multi-headed attention.

24
00:01:17,102 --> 00:01:19,116
What is that and why does it make sense?

25
00:01:19,116 --> 00:01:23,276
And why did the transformer architecture really take off?

26
00:01:23,276 --> 00:01:27,038
I think attention had been around for a long time, but actually thought it was,

27
00:01:27,038 --> 00:01:29,788
one of the things that really made to take off was it allowed

28
00:01:29,788 --> 00:01:32,092
attention to work in a massively parallel way.

29
00:01:32,092 --> 00:01:35,628
So it made it work on modern GPUs and could scale it up.

30
00:01:35,628 --> 00:01:40,274
I think these nuances around transformers are not well-understood by many, so

31
00:01:40,274 --> 00:01:43,188
looking forward to when you deep dive into that.

32
00:01:43,188 --> 00:01:45,353
Absolutely, I mean, the scale is part of it and

33
00:01:45,353 --> 00:01:47,220
how it's able to take in all that data.

34
00:01:47,220 --> 00:01:50,820
I just want to say as well, though, that we're not going to go into this at such

35
00:01:50,820 --> 00:01:53,370
a level which is going to make people's heads explode.

36
00:01:53,370 --> 00:01:56,644
If they want to do that, then they can go ahead and read that paper too.

37
00:01:56,644 --> 00:02:00,302
What we're going to do is we're going to look at the really important

38
00:02:00,302 --> 00:02:04,528
parts of that transformer architecture that gives you the intuition you need so

39
00:02:04,528 --> 00:02:08,490
that you can actually make practical use out of these models.

40
00:02:08,490 --> 00:02:12,775
One thing I've been surprised and delighted by is how transformers,

41
00:02:12,775 --> 00:02:17,566
even though this course focuses on text, it's been really interesting to see

42
00:02:17,566 --> 00:02:21,850
how that basic transformer architecture is creating a foundation for

43
00:02:21,850 --> 00:02:23,792
vision transformers as well.

44
00:02:23,792 --> 00:02:27,986
So even though in this course you learn mostly about large language models,

45
00:02:27,986 --> 00:02:32,378
models about text, I think understanding transformers is also helping people

46
00:02:32,378 --> 00:02:37,092
understand this really exciting vision transformer and other modalities as well.

47
00:02:37,092 --> 00:02:39,313
It's going to be a really critical building block for

48
00:02:39,313 --> 00:02:40,660
a lot of machine learning.

49
00:02:40,660 --> 00:02:41,396
Absolutely.

50
00:02:41,396 --> 00:02:46,688
And then beyond transformers, there's a second major topic that looking forward

51
00:02:46,688 --> 00:02:51,934
to having this first week cover, which is the Generative AI project Lifecycle.

52
00:02:51,934 --> 00:02:56,216
I know a lot of people are thinking, boy, does all this LM stuff, what I do of it?

53
00:02:56,216 --> 00:03:01,599
And the Generative AI project Lifecycle, which will talk about in a little bit,

54
00:03:01,599 --> 00:03:06,810
helps you plan out how to think about building your own Generative AI project.

55
00:03:06,810 --> 00:03:08,059
That's right, and

56
00:03:08,059 --> 00:03:13,056
the Generative AI project Lifecycle walks you through the individual stages and

57
00:03:13,056 --> 00:03:18,140
decisions you have to make when you're developing Generative AI applications.

58
00:03:18,140 --> 00:03:22,033
So one of the first things you have to decide is whether you're taking

59
00:03:22,033 --> 00:03:26,869
a foundation model off the shelf or you're actually pre-training your own model and

60
00:03:26,869 --> 00:03:29,756
then as a follow up, whether you want to fine tune and

61
00:03:29,756 --> 00:03:32,852
customize that model maybe for your specific data.

62
00:03:32,852 --> 00:03:37,045
Yeah, in fact, there's so many large language model options out there,

63
00:03:37,045 --> 00:03:41,567
some open source, some not open source, that I see many developers wondering,

64
00:03:41,567 --> 00:03:43,752
which of these models do I want to use?

65
00:03:43,752 --> 00:03:48,295
And so having a way to evaluate it and then also choose the right model sizing.

66
00:03:48,295 --> 00:03:53,313
I know in your other work, you've talked about when do you need a giant model, 100

67
00:03:53,313 --> 00:03:58,052
billion or even much bigger versus when can a 1 to 30 billion parameter model or

68
00:03:58,052 --> 00:04:03,152
even sub 1 billion parameter model be just fantastic for a specific application?

69
00:04:03,152 --> 00:04:07,750
Exactly, so there might be use cases where you really need the model to be very

70
00:04:07,750 --> 00:04:11,862
comprehensive and able to generalize to a lot of different tasks.

71
00:04:11,862 --> 00:04:14,932
And there might be use cases where you're just optimizing for

72
00:04:14,932 --> 00:04:16,417
a single-use case, right?

73
00:04:16,417 --> 00:04:20,137
And you can potentially work with a smaller model and achieving similar or

74
00:04:20,137 --> 00:04:21,466
even very good results.

75
00:04:21,466 --> 00:04:24,683
Yeah, I think that might be one of the really surprising things for

76
00:04:24,683 --> 00:04:28,072
some people to learn is that you can actually use quite small models and

77
00:04:28,072 --> 00:04:30,488
still get quite a lot of capability out of them.

78
00:04:30,488 --> 00:04:34,420
Yeah, I think when you want your large language model to have a lot of general

79
00:04:34,420 --> 00:04:38,290
knowledge about the world, when you wanted to know stuff about history and

80
00:04:38,290 --> 00:04:42,060
philosophy and the sizes and how to write Python code and so on and so on.

81
00:04:42,060 --> 00:04:46,543
It helps to have a giant model with hundreds of billions of parameters.

82
00:04:46,543 --> 00:04:49,680
But for a single task like summarizing dialogue or

83
00:04:49,680 --> 00:04:54,833
acting as a customer service agent for one company, for applications like that,

84
00:04:54,833 --> 00:04:59,027
sometimes you can use hundreds of billions of parameters models.

85
00:04:59,027 --> 00:05:01,114
But that's not always necessary.

86
00:05:01,114 --> 00:05:05,508
So lots of really exciting material to get into this week.

87
00:05:05,508 --> 00:05:10,342
With that, let's go on to the next video when Mike will kick things off with

88
00:05:10,342 --> 00:05:14,730
a deep dive into many different use cases of large language models.
